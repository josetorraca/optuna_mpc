Python script completed
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to /rds/general/user/jrn23/home/saferl-pcgym/optuna-mpc//runs/ppo/PPO_1
Eval num_timesteps=100, episode_reward=-0.02 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -0.0219  |
| time/              |          |
|    total_timesteps | 100      |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.273   |
| time/              |          |
|    fps             | 70       |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 128      |
---------------------------------
Eval num_timesteps=200, episode_reward=-0.03 +/- 0.00
Episode length: 100.00 +/- 0.00
------------------------------------------------
| eval/                        |               |
|    mean_ep_length            | 100           |
|    mean_reward               | -0.03         |
| time/                        |               |
|    total_timesteps           | 200           |
| train/                       |               |
|    approx_kl                 | 0.00023727864 |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    entropy_loss              | -1.42         |
|    explained_variance        | -61           |
|    learning_rate             | 0.00035       |
|    loss                      | 0.0368        |
|    n_updates                 | 10            |
|    policy_gradient_loss      | -0.0011       |
|    std                       | 0.997         |
|    value_function            | -0.0864       |
|    value_function_derivative | -0.000607     |
|    value_loss                | 0.142         |
------------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.268   |
| time/              |          |
|    fps             | 76       |
|    iterations      | 2        |
|    time_elapsed    | 3        |
|    total_timesteps | 256      |
---------------------------------
Eval num_timesteps=300, episode_reward=-0.04 +/- 0.00
Episode length: 100.00 +/- 0.00
------------------------------------------------
| eval/                        |               |
|    mean_ep_length            | 100           |
|    mean_reward               | -0.0397       |
| time/                        |               |
|    total_timesteps           | 300           |
| train/                       |               |
|    approx_kl                 | 0.00020637503 |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    entropy_loss              | -1.42         |
|    explained_variance        | -8.28         |
|    learning_rate             | 0.000377      |
|    loss                      | 0.00328       |
|    n_updates                 | 20            |
|    policy_gradient_loss      | -0.00115      |
|    std                       | 0.996         |
|    value_function            | -0.114        |
|    value_function_derivative | -0.00229      |
|    value_loss                | 0.0445        |
------------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.292   |
| time/              |          |
|    fps             | 83       |
|    iterations      | 3        |
|    time_elapsed    | 4        |
|    total_timesteps | 384      |
---------------------------------
Eval num_timesteps=400, episode_reward=-0.05 +/- 0.00
Episode length: 100.00 +/- 0.00
------------------------------------------------
| eval/                        |               |
|    mean_ep_length            | 100           |
|    mean_reward               | -0.0534       |
| time/                        |               |
|    total_timesteps           | 400           |
| train/                       |               |
|    approx_kl                 | 0.00097024255 |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    entropy_loss              | -1.41         |
|    explained_variance        | -2.48         |
|    learning_rate             | 0.000391      |
|    loss                      | -0.00784      |
|    n_updates                 | 30            |
|    policy_gradient_loss      | -0.00309      |
|    std                       | 0.988         |
|    value_function            | -0.157        |
|    value_function_derivative | -0.000898     |
|    value_loss                | 0.011         |
------------------------------------------------
Eval num_timesteps=500, episode_reward=-0.05 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -0.0534  |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.263   |
| time/              |          |
|    fps             | 74       |
|    iterations      | 4        |
|    time_elapsed    | 6        |
|    total_timesteps | 512      |
---------------------------------
Eval num_timesteps=600, episode_reward=-0.07 +/- 0.00
Episode length: 100.00 +/- 0.00
-----------------------------------------------
| eval/                        |              |
|    mean_ep_length            | 100          |
|    mean_reward               | -0.072       |
| time/                        |              |
|    total_timesteps           | 600          |
| train/                       |              |
|    approx_kl                 | 0.0019323956 |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    entropy_loss              | -1.4         |
|    explained_variance        | -1.35        |
|    learning_rate             | 0.000397     |
|    loss                      | -0.00614     |
|    n_updates                 | 40           |
|    policy_gradient_loss      | -0.00369     |
|    std                       | 0.983        |
|    value_function            | -0.107       |
|    value_function_derivative | 0.00466      |
|    value_loss                | 0.00763      |
-----------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.193   |
| time/              |          |
|    fps             | 79       |
|    iterations      | 5        |
|    time_elapsed    | 8        |
|    total_timesteps | 640      |
---------------------------------
Eval num_timesteps=700, episode_reward=-0.09 +/- 0.00
Episode length: 100.00 +/- 0.00
-----------------------------------------------
| eval/                        |              |
|    mean_ep_length            | 100          |
|    mean_reward               | -0.0877      |
| time/                        |              |
|    total_timesteps           | 700          |
| train/                       |              |
|    approx_kl                 | 0.0011718632 |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    entropy_loss              | -1.4         |
|    explained_variance        | -2.36        |
|    learning_rate             | 0.000399     |
|    loss                      | -0.0126      |
|    n_updates                 | 50           |
|    policy_gradient_loss      | -0.002       |
|    std                       | 0.979        |
|    value_function            | -0.0741      |
|    value_function_derivative | -0.000436    |
|    value_loss                | 0.0022       |
-----------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.265   |
| time/              |          |
|    fps             | 82       |
|    iterations      | 6        |
|    time_elapsed    | 9        |
|    total_timesteps | 768      |
---------------------------------
Eval num_timesteps=800, episode_reward=-0.07 +/- 0.00
Episode length: 100.00 +/- 0.00
-----------------------------------------------
| eval/                        |              |
|    mean_ep_length            | 100          |
|    mean_reward               | -0.071       |
| time/                        |              |
|    total_timesteps           | 800          |
| train/                       |              |
|    approx_kl                 | 0.0010439744 |
|    clip_fraction             | 0            |
|    clip_range                | 0.2          |
|    entropy_loss              | -1.4         |
|    explained_variance        | -1.73        |
|    learning_rate             | 0.0004       |
|    loss                      | -0.00927     |
|    n_updates                 | 60           |
|    policy_gradient_loss      | -0.00282     |
|    std                       | 0.983        |
|    value_function            | -0.0841      |
|    value_function_derivative | -0.000392    |
|    value_loss                | 0.00176      |
-----------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.123   |
| time/              |          |
|    fps             | 84       |
|    iterations      | 7        |
|    time_elapsed    | 10       |
|    total_timesteps | 896      |
---------------------------------
Eval num_timesteps=900, episode_reward=-0.08 +/- 0.00
Episode length: 100.00 +/- 0.00
------------------------------------------------
| eval/                        |               |
|    mean_ep_length            | 100           |
|    mean_reward               | -0.0823       |
| time/                        |               |
|    total_timesteps           | 900           |
| train/                       |               |
|    approx_kl                 | 0.00059620617 |
|    clip_fraction             | 0             |
|    clip_range                | 0.2           |
|    entropy_loss              | -1.4          |
|    explained_variance        | -0.961        |
|    learning_rate             | 0.0004        |
|    loss                      | -0.023        |
|    n_updates                 | 70            |
|    policy_gradient_loss      | -0.00269      |
|    std                       | 0.987         |
|    value_function            | -0.0665       |
|    value_function_derivative | 0.00127       |
|    value_loss                | 0.00178       |
------------------------------------------------
Eval num_timesteps=1000, episode_reward=-0.08 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -0.0823  |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -0.171   |
| time/              |          |
|    fps             | 79       |
|    iterations      | 8        |
|    time_elapsed    | 12       |
|    total_timesteps | 1024     |
---------------------------------
Python script completed
